# coding=utf-8
import os
import logging
import sys
import re
import multiprocessing
import gensim

from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

def process_wiki(inp, outp):
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)

    i = 0

    #output = open(outp, 'w', encoding='utf-8')
    output = open(outp, 'wt', encoding='utf-8')#ä½¿ç”¨ â€œtâ€?ç±»å‹æ‰“å¼€æ–‡ä»¶ï¼Œå­—ç¬¦ä¸²çš„å½¢å¼?
    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        #output.write(b' '.join(text).decode('utf-8') + '\n')
        output.write( " ".join('%s' %id for id in text) + '\n')#éå†listçš„å…ƒç´ ï¼ŒæŠŠä»–è½¬åŒ–æˆå­—ç¬¦ä¸²ã€?
        #output.write('\n')
        i = i + 1
        if i % 10000 == 0:
            logger.info('Saved ' + str(i) + ' articles')

    output.close()
    logger.info('Finished ' + str(i) + ' articles')

def remove_words(inp, outp):
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)

    output = open(outp, 'w', encoding='utf-8')
    inp = open(inp, 'r', encoding='utf-8')

    for line in inp.readlines():
        ss = re.findall('[\n\s*\r\u4e00-\u9fa5]', line)
        output.write("".join(ss))
    logger.info("Finished removed words!")

def separate_words(inp, outp):
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)


    output = open(outp, 'w', encoding='utf-8')
    inp = open(inp, 'r', encoding='utf-8')

    for line in inp.readlines():
        seg_list = jieba.cut(line.strip())
        output.write(' '.join(seg_list) + '\n')
    logger.info("finished separate words!")

# inpä¸ºè¾“å…¥è¯­æ–?
def train_w2v_model(inp, outp1, outp2):
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info("running %s" % ' '.join(sys.argv))

    # LineSentence(inp)ï¼šåº”è¯¥æ˜¯æŠŠword2vecè®­ç»ƒæ¨¡å‹çš„ç£ç›˜å­˜å‚¨æ–‡ä»¶ï¼ˆmodelåœ¨å†…å­˜ä¸­æ€»æ˜¯ä¸è¸å®ï¼‰è½¬æ¢æˆæ‰€éœ€è¦çš„æ ¼å¼ï¼›å¯¹åº”çš„æ ¼å¼æ˜¯å‚è€ƒä¸Šé¢çš„ä¾?ã€?
    # sizeï¼šæ˜¯æ¯ä¸ªè¯çš„å‘é‡ç»´åº¦ï¼?
    # windowï¼šæ˜¯è¯å‘é‡è®­ç»ƒæ—¶çš„ä¸Šä¸‹æ–‡æ‰«æçª—å£å¤§å°ï¼Œçª—å£ä¸º5å°±æ˜¯è€ƒè™‘å‰?ä¸ªè¯å’Œå5ä¸ªè¯ï¼?
    # min - countï¼šè®¾ç½®æœ€ä½é¢‘ç‡ï¼Œé»˜è®¤æ˜?ï¼Œå¦‚æœä¸€ä¸ªè¯è¯­åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°å°äº5ï¼Œé‚£ä¹ˆå°±ä¼šä¸¢å¼ƒï¼›
    # workersï¼šæ˜¯è®­ç»ƒçš„è¿›ç¨‹æ•°ï¼ˆéœ€è¦æ›´ç²¾å‡†çš„è§£é‡Šï¼Œè¯·æŒ‡æ­£ï¼‰ï¼Œé»˜è®¤æ˜¯å½“å‰è¿è¡Œæœºå™¨çš„å¤„ç†å™¨æ ¸æ•°ã€‚è¿™äº›å‚æ•°å…ˆè®°ä½å°±å¯ä»¥äº†ã€?

    model = Word2Vec(LineSentence(inp), size=100, window=5, min_count=2,
                     workers=multiprocessing.cpu_count())

    # outp1 ä¸ºè¾“å‡ºæ¨¡å?
    model.save(outp1)

    # outp2ä¸ºåŸå§‹cç‰ˆæœ¬word2vecçš„vectoræ ¼å¼çš„æ¨¡å?
    model.wv.save_word2vec_format(outp2, binary=False)

def main():
    #process_wiki('enwiki-latest-pages-articles.xml.bz2', 'wiki.en.text')
    #process_wiki('enwiki-20180320-pages-articles14.xml-p7697599p7744799.bz2', 'wiki.en.text')
    # remove_words('./data/wiki_cn_jian.txt', './data/wiki_cn_jian_removed.txt')
    # separate_words('./data/wiki_cn_jian_removed.txt', './data/wiki_cn_jian_sep_removed.txt')
    # train_w2v_model('./data/wiki_cn_jian_sep_removed.txt', './bin/300/w2v_model.bin', './bin/300/w2v_vector.bin')
    train_w2v_model('wiki.en.text', 'w2v_model_100.bin', 'w2v_vector_100.bin')


if __name__=='__main__':
    main()
    # model = gensim.models.Word2Vec.load('./bin/300/w2v_model.bin')
    # print(model.most_similar([u'æè¿æ?, u'åŸºé‡‘'], [u'æˆé¾™']))